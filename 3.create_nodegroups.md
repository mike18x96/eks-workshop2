# Unmanaged node groups
Update cluster.yaml from the provious step or create new.

```
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: basic-cluster
  region: us-west-2
  version: "1.25"

nodeGroups:
  - name: ng1
    labels: { role: workers }
    instanceType: t3.small
    desiredCapacity: 1
    volumeSize: 20
    privateNetworking: true
```

Here we have added unmanaged, private nodegroup with one instance of type t3.small

If you have previously created cluster, please run:
```
eksctl create nodegroup --config-file=cluster.yaml
```

If you don't have cluster run cluster creation and nodegroup from scratch
```
eksctl create cluster -f cluster.yaml
```

Meanwhile eksctl will create CloudFormation stack with resouces managed with autoscaling group

Check if node is connected to the cluster
```
 kubectl get nodes
```


# Managed node group
Now let's add managed node group
Amazon EKS managed nodegroups is a feature that automates the provisioning and lifecycle management of nodes (EC2 instances) for Amazon EKS Kubernetes clusters. Customers can provision optimized groups of nodes for their clusters and EKS will keep their nodes up to date with the latest Kubernetes and host OS versions. 

An EKS managed node group is an autoscaling group and associated EC2 instances that are managed by AWS for an Amazon EKS cluster. Each node group uses the Amazon EKS-optimized Amazon Linux 2 AMI. Amazon EKS makes it easy to apply bug fixes and security patches to nodes, as well as update them to the latest Kubernetes versions. Each node group launches an autoscaling group for your cluster, which can span multiple AWS VPC availability zones and subnets for high-availability.
```
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: basic-cluster
  region: us-west-2
  version: "1.25"

nodeGroups:
  - name: ng1
    labels: { role: workers }
    instanceType: t3.small
    desiredCapacity: 1
    volumeSize: 20
    private
managedNodeGroups:
  - name: ng2
    instanceType: t3.micro
    minSize: 1
    desiredCapacity: 1
    maxSize: 4
    labels:
      role: managed-worker
    tags:
      nodegroup-name: ng2
    privateNetworking: true
    ssh: 
      enableSsm: true
```


```
eksctl create nodegroup --config-file=cluster.yaml
```


# Find right instance
Sometimes it's hard to find what ec2 instantce type do we really need. to make this easier task eksctl integrates with ec2 instance selector. Let's try to deploy nodegroup using that feature.

let's run following command in dry run mode to generate cluster manifest:
```
eksctl create cluster --name dev-cluster --managed --instance-selector-vcpus=2 --instance-selector-memory=4 --dry-run
```


# Spot and mixed

edit cluster.yaml

```
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: basic-cluster
  region: us-west-2
  version: "1.25"

nodeGroups:
  - name: ng1
    labels: { role: workers }
    instanceType: t3.small
    desiredCapacity: 1
    volumeSize: 20
    privateNetworking: true
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: basic-cluster
  region: us-west-2
  version: "1.25"

nodeGroups:
  - name: ng1
    labels: { role: workers }
    instanceType: t3.small
    desiredCapacity: 1
    volumeSize: 20
    privateNetworking: true
  - name: ng3 # 50% spot, 50% on-demand
    minSize: 1
    maxSize: 3
    instancesDistribution:
      maxPrice: 0.017
      instanceTypes: ["t3.small", "t3.medium"] # At least one instance type should be specified
      onDemandBaseCapacity: 0
      onDemandPercentageAboveBaseCapacity: 50
      spotInstancePools: 2
managedNodeGroups:
  - name: ng2
    instanceType: t3.micro
    minSize: 1
    desiredCapacity: 1
    maxSize: 4
    labels:
      role: managed-worker
    tags:
      nodegroup-name: ng2
    privateNetworking: true
    ssh: 
      enableSsm: true
managedNodeGroups:
  - name: ng2
    instanceType: t3.micro
    minSize: 1
    desiredCapacity: 1
    maxSize: 4
    labels:
      role: managed-worker
    tags:
      nodegroup-name: ng2
    privateNetworking: true
    ssh: 
      enableSsm: true
```

deploy nodegroup:
```
eksctl create nodegroup -f cluster.yaml
```

# Nodegroup immutability
By design, nodegroups are immutable. This means that if you need to change something (other than scaling) like the AMI or the instance type of a nodegroup, you would need to create a new nodegroup with the desired changes, move the load and delete the old one.

# Delete nodegroup
eksctl delete nodegroup --cluster=clusterName --name=nodegroupName
  
Let's run:
```
eksctl delete nodegroup --cluster=basic-cluster --name=ng3
```
Check if node group is gone:
```
kubectl get nodes
```
You can also remove nodegroup configuration from cluster.yaml to keep it up to date.

# Draining
 All nodes are cordoned(k8s make it unschedulable, so this command applies a taint on this node to mark it as NoSchedule) and all pods are evicted from a nodegroup on deletion, but if you need to drain a nodegroup without deleting it, run:
```
eksctl drain nodegroup --cluster=<clusterName> --name=<nodegroupName>
```

# Additional eksctl example
https://github.com/weaveworks/eksctl/tree/main/examples
